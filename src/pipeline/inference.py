# --- src/pipeline/inference.py ---

import torch
from transformers import AutoProcessor, Gemma3nForConditionalGeneration, BitsAndBytesConfig
from PIL import Image
import os

# --- Configuration ---
MODEL_ID = "google/gemma-3n-e2b-it" # Using the efficient 2B instruction-tuned variant

# --- Model Loading (with caching) ---
# We define model and processor as global variables so they are loaded only once.
model = None
processor = None

def load_model():
    """
    Loads the quantized Gemma 3n model and processor.
    This function is called once to initialize the model in memory.
    """
    global model, processor

    if model is None:
        print("Loading Gemma 3n model for the first time...")
        
        # Configure 4-bit quantization to reduce memory footprint
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True
        )

        try:
            # Use a local cache directory to avoid re-downloading
            cache_dir = "./model_cache"
            model = Gemma3nForConditionalGeneration.from_pretrained(
                MODEL_ID,
                quantization_config=quantization_config,
                torch_dtype=torch.bfloat16, # Using bfloat16 for performance
                cache_dir=cache_dir
            )
            processor = AutoProcessor.from_pretrained(MODEL_ID, cache_dir=cache_dir)
            print("Model and processor loaded successfully.")
        except Exception as e:
            print(f"Error loading model: {e}")
            raise

def get_gemma_diagnosis(image_path: str, user_query: str) -> str:
    """
    Generates a diagnosis for a given plant image and user query.

    Args:
        image_path (str): The local path to the plant image file.
        user_query (str): The user's question about the plant.

    Returns:
        str: The model's generated diagnosis and remedy.
    """
    # Ensure model is loaded
    if model is None or processor is None:
        load_model()
        if model is None: # Check if loading failed
            return "Error: Model could not be loaded. Please check logs."

    # Verify image path
    if not os.path.exists(image_path):
        return "Error: Image file not found at the specified path."

    try:
        # Open the image file
        image = Image.open(image_path).convert("RGB")

        # Construct the prompt for the model
        # This format is crucial for instruction-tuned models
        prompt = f"<image>\n<start_of_turn>user\n{user_query}<end_of_turn>\n<start_of_turn>model\n"

        # Process the inputs
        inputs = processor(text=prompt, images=image, return_tensors="pt").to(model.device)

        # Generate a response
        generation_output = model.generate(**inputs, max_new_tokens=250, do_sample=False)
        
        # Decode and clean the output
        response_text = processor.decode(generation_output[0], skip_special_tokens=True)
        # Clean up the output to only get the model's response part
        clean_response = response_text.split("model\n")[-1].strip()

        return clean_response

    except Exception as e:
        print(f"An error occurred during inference: {e}")
        return "Sorry, an error occurred while analyzing the image."

# --- src/pipeline/uncertainty.py ---

def is_uncertain(response: str) -> bool:
    """
    Checks if the model's response indicates uncertainty.

    Args:
        response (str): The text generated by the model.

    Returns:
        bool: True if uncertain, False otherwise.
    """
    # Placeholder for uncertainty logic
    # We will implement the heuristics here in Week 3
    pass

# --- src/rag/build_index.py ---

def build_and_save_index(data_path: str, save_path: str):
    """
    Builds a FAISS index from text data and saves it to disk.

    Args:
        data_path (str): Path to the text data (e.g., CSV file).
        save_path (str): Path to save the generated FAISS index.
    """
    # Placeholder for building the FAISS index
    # We will implement this in Week 2
    pass

# --- src/rag/search.py ---

def search_knowledge_base(query: str, index_path: str, top_k: int = 3) -> list:
    """
    Searches the FAISS index for the most relevant documents.

    Args:
        query (str): The user's query text.
        index_path (str): Path to the FAISS index file.
        top_k (int): The number of top results to return.

    Returns:
        list: A list of the most relevant text chunks.
    """
    # Placeholder for searching the FAISS index
    # We will implement this in Week 2
    pass

# --- src/utils/audio_processing.py ---

def transcribe_audio(audio_file_path: str) -> str:
    """
    Transcribes an audio file to text using Whisper.

    Args:
        audio_file_path (str): Path to the audio file.

    Returns:
        str: The transcribed text.
    """
    # Placeholder for Whisper transcription
    # We will implement this later in Week 1
    pass

def text_to_speech(text: str, lang: str = 'hi') -> str:
    """
    Converts text to speech and returns the path to the audio file.

    Args:
        text (str): The text to convert.
        lang (str): The language code (e.g., 'hi' for Hindi).

    Returns:
        str: Path to the saved audio file.
    """
    # Placeholder for gTTS conversion
    # We will implement this later in Week 1
    pass

